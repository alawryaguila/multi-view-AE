# @package _global_

# TODO: some variables are "required" to be present always (regardless of value), how to enforce this?
# TODO: user can replace variables but cannot remove...
# TODO: _target_ should not be replaceable except for enc/dec _target_ and enc_dist/dec_dist _target_
# TODO: user should be able to replace _target_ with classnames and not full classpath

defaults:
  - _self_
  - model_type: null

out_dir: ${hydra.runtime.cwd}/outputs/${model_name}/${now:%Y-%m-%d_%H%M}

hydra:
  # TODO: this does not work... I want to save yaml files, i think it is not enabled when using initialize/compose
  output_subdir: ${out_dir}/.hydra
  run:
    dir: ${out_dir}

model:
  use_GPU: False
  save_model: True

  seed_everything: True
  seed: 42      # TODO: if not given, randomly choose and save?? needs to be saved in config file

  # TODO: does not support single view
  # TODO: support multi-dimensional tuple
  z_dim: 5
  learning_rate: 0.001  # learning rate for both encoder/decoder

  sparse: False
  threshold: 0.2

datamodule:
  _target_: multiae.base.dataloaders.MultiviewDataModule

  # TODO: specify in docs what happens if batch_size is None
  batch_size: null
  is_validate: False

  train_size: 0.9

encoder:
  _target_: multiae.models.layers.Encoder

  hidden_layer_dim: []  # TODO: why empty default?
  bias: True

decoder:
  _target_: multiae.models.layers.Decoder

  # TODO: test hidden_layer_dim not same as encoder's?
  hidden_layer_dim: []  # TODO: note in documentation that this will be reversed... usually same as encoder's hidden layers
  bias: True

  dec_dist:
    _target_: multiae.base.distributions.Default

trainer:
  _target_: pytorch_lightning.Trainer

  gpus: 0

  max_epochs: 10

  deterministic: false
  log_every_n_steps: 2

  # NOTE: null means do not resume_from_checkpoint
  resume_from_checkpoint: null #${out_dir}/last.ckpt  #TODO: should default be to resume?

callbacks:
  # TODO: val_loss does not work if is_validation = False
  # should override this when is_validation = True??
  model_checkpoint:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: "train_loss"
    mode: "min"
    save_last: True
    dirpath: ${out_dir}

  early_stopping:
    _target_: pytorch_lightning.callbacks.EarlyStopping
    monitor: "train_loss"
    mode: "min"
    patience: 100
    min_delta: 0.001
    verbose: True

logger:
  _target_: pytorch_lightning.loggers.tensorboard.TensorBoardLogger

  save_dir: ${out_dir}/logs
