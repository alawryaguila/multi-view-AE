# @package _global_

defaults:
  - _self_
  - model_type: null


out_dir: ${hydra.runtime.cwd}/outputs/${model_name}/${now:%Y-%m-%d_%H%M}

model:
  use_GPU: False #whether to use a GPU if available
  save_model: False #whether to save the model

  seed_everything: True
  seed: 42

  z_dim: 5 #dimensionality of the latent space
  learning_rate: 0.001  # learning rate for both encoder/decoder

  #sparsity of the encoding distribution.
  sparse: False  #NOTE: setting sparse to True enforces a uniform prior over the latents
  threshold: 0

datamodule:
  _target_: multiae.base.dataloaders.MultiviewDataModule

  batch_size: null
  is_validate: True

  train_size: 0.9

encoder:  # uses default mlp for all inputs
  default:
      _target_: multiae.architectures.mlp.Encoder

      hidden_layer_dim: []
      bias: True  # only applies to linear layers
      non_linear: False

      enc_dist:
        _target_: multiae.base.distributions.Default

decoder: # uses default mlp for all inputs
  default:
      _target_: multiae.architectures.mlp.Decoder

      hidden_layer_dim: []
      bias: True  # only applies to linear layers
      non_linear: False

      dec_dist:
        _target_: multiae.base.distributions.Default

prior:
  _target_: multiae.base.distributions.Normal
  loc: 0
  scale: 1

trainer:
  _target_: pytorch_lightning.Trainer

  gpus: 0

  max_epochs: 10

  deterministic: False
  log_every_n_steps: 2

  resume_from_checkpoint: null #${out_dir}/last.ckpt

callbacks:
  model_checkpoint:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: "val_loss"
    mode: "min"
    save_last: True
    dirpath: ${out_dir}

  early_stopping:
    _target_: pytorch_lightning.callbacks.EarlyStopping
    monitor: "val_loss"
    mode: "min"
    patience: 50
    min_delta: 0.001
    verbose: True

logger: #TODO: check other logger frameworks work
  _target_: pytorch_lightning.loggers.tensorboard.TensorBoardLogger

  save_dir: ${out_dir}/logs
